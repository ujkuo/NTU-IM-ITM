{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = 'The faster Harry got to the store, the faster Harry, the fater, would get home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'the',\n",
       " 'fater',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(doc)\n",
    "stop_punc = [',',';','.']\n",
    "final_tokens = [x for x in tokens if x not in stop_punc]\n",
    "final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 3), ('faster', 2), ('Harry', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(final_tokens)\n",
    "bag_of_words.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['The faster Harry got to the store, the faster and faster Harry would get home.', 'Harry is hairy and faster than Jill.','Jill is not as hairy as Harry.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'faster', 'Harry', 'got', 'to', 'the', 'store', 'the', 'faster', 'and', 'faster', 'Harry', 'would', 'get', 'home'], ['Harry', 'is', 'hairy', 'and', 'faster', 'than', 'Jill'], ['Jill', 'is', 'not', 'as', 'hairy', 'as', 'Harry']]\n"
     ]
    }
   ],
   "source": [
    "stop_punc = [',',';','.']\n",
    "docs_tokens = []\n",
    "for doc in docs:\n",
    "    tokens = word_tokenize(doc)\n",
    "    final_tokens = [x for x in tokens if x not in stop_punc]\n",
    "    docs_tokens += [final_tokens]\n",
    "    \n",
    "print(docs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " 'Harry',\n",
       " 'is',\n",
       " 'hairy',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'than',\n",
       " 'Jill',\n",
       " 'Jill',\n",
       " 'is',\n",
       " 'not',\n",
       " 'as',\n",
       " 'hairy',\n",
       " 'as',\n",
       " 'Harry']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs_tokens = []\n",
    "for each_doc_tokens in docs_tokens:\n",
    "    all_docs_tokens.extend(each_doc_tokens)\n",
    "    \n",
    "all_docs_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry', 'Jill', 'The', 'and', 'as', 'faster', 'get', 'got', 'hairy', 'home', 'is', 'not', 'store', 'than', 'the', 'to', 'would']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(all_docs_tokens))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_docs = len(docs)\n",
    "TF_vectors = np.zeros((num_docs, vocab_size), float)\n",
    "print(TF_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Counter({'faster': 3, 'Harry': 2, 'the': 2, 'The': 1, 'got': 1, 'to': 1, 'store': 1, 'and': 1, 'would': 1, 'get': 1, 'home': 1})\n",
      "7\n",
      "Counter({'Harry': 1, 'is': 1, 'hairy': 1, 'and': 1, 'faster': 1, 'than': 1, 'Jill': 1})\n",
      "7\n",
      "Counter({'as': 2, 'Jill': 1, 'is': 1, 'not': 1, 'hairy': 1, 'Harry': 1})\n"
     ]
    }
   ],
   "source": [
    "for i, token_list in enumerate(docs_tokens):\n",
    "    doc_len = len(token_list)\n",
    "    print(doc_len)\n",
    "    token_counts = Counter(token_list)\n",
    "    print(token_counts)\n",
    "    for key, value in token_counts.items():\n",
    "        TF_vectors[i][vocab.index(key)] = value / doc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.133 0.    0.067 0.067 0.    0.2   0.067 0.067 0.    0.067 0.    0.\n",
      "  0.067 0.    0.133 0.067 0.067]\n",
      " [0.143 0.143 0.    0.143 0.    0.143 0.    0.    0.143 0.    0.143 0.\n",
      "  0.    0.143 0.    0.    0.   ]\n",
      " [0.143 0.143 0.    0.    0.286 0.    0.    0.    0.143 0.    0.143 0.143\n",
      "  0.    0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(TF_vectors.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_vectorizer = CountVectorizer()\n",
    "TF_vectors = TF_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TF_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 3 1 1 0 2 1 0 0 0 1 0 3 1 1]\n",
      " [1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0]\n",
      " [0 2 0 0 0 1 1 0 1 1 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(TF_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1614879  0.         0.48446369 0.21233718 0.21233718 0.\n",
      "  0.25081952 0.21233718 0.         0.         0.         0.21233718\n",
      "  0.         0.63701154 0.21233718 0.21233718]\n",
      " [0.36930805 0.         0.36930805 0.         0.         0.36930805\n",
      "  0.28680065 0.         0.36930805 0.36930805 0.         0.\n",
      "  0.48559571 0.         0.         0.        ]\n",
      " [0.         0.75143242 0.         0.         0.         0.28574186\n",
      "  0.22190405 0.         0.28574186 0.28574186 0.37571621 0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TFIDF_vectorizer = TfidfVectorizer()\n",
    "TFIDF_vectors = TFIDF_vectorizer.fit_transform(docs)\n",
    "print(TFIDF_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.42111744, 0.12379689],\n",
       "       [0.42111744, 1.        , 0.50395263],\n",
       "       [0.12379689, 0.50395263, 1.        ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(TF_vectors, TF_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.31049032, 0.05565787],\n",
       "       [0.31049032, 1.        , 0.38022254],\n",
       "       [0.05565787, 0.38022254, 1.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(TFIDF_vectors, TFIDF_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055657865713652735"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(TFIDF_vectors[0], TFIDF_vectors[2]).flatten()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
